Text generated by copilot
# Overview of Tools, Workflow, and Architecture

**ft_transcendence** is the final milestone of the 42 Common Core, to be developed by a team of 4 to 5 students. It simulates a real-world professional environment by intentionally confronting students with unfamiliar technologies and strict constraints. The project's core purpose is to prove students' ability to adapt, learn new tools quickly, and manage a complex software architecture.


## Table of Contents
- [Prisma](#prisma)
- [Migration](#migration)
- [Swagger](#swagger)
- [My API ?](#my-api-)
- [Fastify and Vite](#fastify-and-vite)
- [Vite setup](#vite-setup)
- [Cookie](#cookie)
- [Caddy](#caddy)
- [WebSockets](#websockets)
- [JWT](#jwt)
- [CI/CD](#cicd)
- [Pre-commit Hooks](#pre-commit-hooks)
- [Coverage requirements](#coverage-requirements)
- [Dockerfile stages](#dockerfile-stages)
- [Github Actions](#github-actions)
- [Dependabot](#dependabot)
- [Running the project on 42 school computer](#running-the-project-on-42-school-computer)
- [Pro handling of a Pull Request](#pro-handling-of-a-pull-request)

---

Entry-point of frontend:  
At the bottom of the file frontend/main.ts, `document.addEventListener('DOMContentLoaded', () => {...}`

Entry-point of backend:  
backend/src/index.ts  

---

# JS architecture

JavaScript utilise une architecture non-bloquante bas√©e sur l'Event Loop.
Comment √ßa marche techniquement ?

Mise en pause locale : Quand le moteur rencontre await, il "suspend" l'ex√©cution de la fonction async actuelle. Il sauvegarde son √©tat (variables, curseur d'ex√©cution).

Lib√©ration du thread : La fonction est retir√©e de la Call Stack (la pile d'ex√©cution). Le thread principal (unique en JS) est d√©sormais vide.

Gestion des autres t√¢ches : Le moteur peut maintenant piocher dans la Task Queue (file d'attente) pour traiter :
- Des √©v√©nements DOM (clics, scroll).
- Des timers (setTimeout).
- D'autres requ√™tes r√©seau.
- Le rendu graphique (UI).
Reprise : Une fois que la Promise attendue est r√©solue, la suite de la fonction est plac√©e dans la Microtask Queue pour √™tre r√©inject√©e dans la Call Stack d√®s que celle-ci est libre.

```js
async function blockMe() {
  console.log("D√©but fetch");
  await fetch('https://api.longue-reponse.com'); // Peut durer 5 secondes
  console.log("Fin fetch");
}

blockMe();

// Cette partie reste interactive m√™me pendant les 5s du fetch :
document.addEventListener('click', () => console.log("Clic d√©tect√© !"));
```
L'interface reste fluide car le moteur traite l'√©v√©nement click pendant que la requ√™te r√©seau tourne en arri√®re-plan.  
await bloque la progression logique de votre script (l'ordre des lignes), mais ne bloque pas la progression physique du thread JavaScript.  

Il faut voir le moteur (V8) non pas comme un bloc isol√©, mais comme un chef d'orchestre dans un environnement (Navigateur ou Node.js).

### Voici les quatre composants qui font fonctionner l'asynchronisme :
#### 1. La Call Stack (La Pile)
C'est l√† que le code est ex√©cut√©. C'est une structure LIFO (Last In, First Out).
- Si vous appelez une fonction, elle est empil√©e.
- Quand elle finit, elle est d√©pil√©e.
- Probl√®me : Elle est mono-thread√©e. Une seule t√¢che √† la fois.

#### 2. Les Web APIs (Le Background)
Ce n'est pas du JavaScript pur, ce sont des outils fournis par le navigateur (ou l'OS via Node).
- fetch, setTimeout, les √©couteurs d'√©v√©nements (DOM).
Quand vous faites un await fetch(), JS d√©l√®gue la requ√™te aux Web APIs et lib√®re imm√©diatement la Call Stack.

#### 3. La Task Queue & Microtask Queue (La Salle d'attente)
Une fois qu'une Web API a termin√© son travail (ex: la r√©ponse r√©seau est arriv√©e), le callback (ou la suite du code apr√®s le await) est plac√© ici.
- Microtask Queue : Priorit√© haute (Promesses, await).
- Task Queue : Priorit√© basse (setTimeout, √©v√©nements DOM).

#### 4. L'Event Loop (Le Surveillant)
Son r√¥le est unique et simple. Il v√©rifie en boucle :
Est-ce que la Call Stack est vide ?
Si oui, il prend la premi√®re t√¢che de la Microtask Queue et l'envoie dans la Call Stack pour l'ex√©cuter.

### Pourquoi c'est vital de le savoir ?
Si vous √©crivez une boucle de calcul intensive (ex: crypter un fichier) directement dans votre code, vous remplissez la Call Stack pendant 10 secondes. L'Event Loop ne pourra jamais envoyer les clics ou les rendus graphiques depuis la file d'attente vers la pile. L'interface g√®le.
#### R√©sum√© du flux async/await:
- await : "Je sors de la Call Stack et je laisse l'API g√©rer √ßa."
- API : Fait le travail en arri√®re-plan.
- Queue : "Le travail est fini, j'attends mon tour."
- Event Loop : "La pile est libre, reviens finir ton ex√©cution."

---

# Technology Stack

This section lists all the technologies used in the project, categorized by their role.

## Backend (`/backend`)

*   **Node.js**: JavaScript runtime built on Chrome's V8 engine, used to run the server.
*   **TypeScript**: A typed superset of JavaScript that compiles to plain JavaScript, adding static types for better code quality.
*   **Fastify**: A high-performance, low-overhead web framework for Node.js. Used as the main server framework instead of Express.
*   **Prisma**: A modern ORM (Object-Relational Mapper) for Node.js and TypeScript. It simplifies database access with a type-safe API.
*   **SQLite (`better-sqlite3`)**: A self-contained, serverless, zero-configuration, transactional SQL database engine. Used as the development database.
*   **@fastify/jwt**: A plugin for Fastify to handle JSON Web Tokens (JWT) for user authentication.
*   **@fastify/oauth2**: A plugin for Fastify to handle OAuth2 flows (e.g., Google Login).
*   **Argon2**: A secure password hashing algorithm. Used to hash user passwords before storing them in the database.
*   **Zod**: A TypeScript-first schema declaration and validation library. Used to validate incoming data (API requests).
*   **Swagger / OpenAPI (`@fastify/swagger`)**: Tools for documenting APIs. Provides an interactive UI to test API endpoints.
*   **Viem**: A lightweight, composable, and type-safe TypeScript interface for Ethereum. Used to interact with the blockchain from the backend.

## Frontend (`/frontend`)

*   **Vite**: A build tool that aims to provide a faster and leaner development experience for modern web projects.
*   **TypeScript**: Used for all frontend logic to ensure type safety.
*   **Tailwind CSS**: A utility-first CSS framework for rapidly building custom user interfaces.
*   **PostCSS**: A tool for transforming CSS with JavaScript plugins (used by Tailwind).
*   **Vitest**: A blazing fast unit test framework powered by Vite. Used for testing frontend logic.
*   **JSDOM**: A JavaScript implementation of various web standards, for use with Node.js. Used to test DOM interactions in Vitest.
*   **Vanilla TypeScript**: The project does not use a UI framework like React or Vue. It uses plain TypeScript and direct DOM manipulation (likely for the game canvas).

## Blockchain (`/blockchain`)

*   **Hardhat**: A development environment for Ethereum software. It consists of different components for editing, compiling, debugging, and deploying smart contracts and dApps.
*   **Solidity**: An object-oriented, high-level language for implementing smart contracts.
*   **Hardhat Ignition**: A declarative deployment system for Hardhat, used to deploy smart contracts.
*   **Viem**: Used here as well for interacting with the blockchain in tests and scripts.
*   **Avalanche Fuji**: The test network (testnet) for the Avalanche blockchain, where contracts are deployed for testing.

## Infrastructure & DevOps

*   **Docker**: A platform for developing, shipping, and running applications in containers.
*   **Docker Compose**: A tool for defining and running multi-container Docker applications (Backend + Frontend + Database).
*   **pnpm**: A fast, disk space-efficient package manager. Used to manage dependencies across the monorepo.
*   **Caddy**: An open-source web server with automatic HTTPS. Likely used as a reverse proxy in production.
*   **ESLint**: A static code analysis tool for identifying problematic patterns in JavaScript code.
*   **Prettier**: An opinionated code formatter.

## Testing

*   **Vitest**: Used across both frontend and backend for unit and integration testing.

---

## Prisma

Prisma is a modern ORM (Object-Relational Mapper).  
Think of it as a "translator" between your code and your database.

- **Without Prisma:** You would have to write raw SQL queries like:
  ```sql
  SELECT * FROM User WHERE email = 'alice@example.com';
  ```
- **With Prisma:** You write TypeScript code that looks like this:
  ```typescript
  const user = await prisma.user.findUnique({
    where: { email: 'alice@example.com' },
  });
  ```

### Why is it used here?

1.  **Type Safety**: Since you are using TypeScript, Prisma automatically generates types for your database models. If you try to access `user.phonenumber` but that field doesn't exist in your schema, your code won't even compile.
2.  **Auto-Completion**: In VS Code, when you type `prisma.user.`, it will show you all the available methods (`create`, `findMany`, `delete`, etc.) and fields.
3.  **Migrations**: It manages changes to your database structure. When you edit schema.prisma (like adding a `score` field), Prisma generates the SQL commands to update your database safely.

In this project, it connects your **Node.js backend** to the **SQLite database**.

More about prisma: https://pris.ly/d/prisma-schema

### Prisma studio
**Prisma Studio** is a visual database editor that runs in your browser.
Think of it as a "spreadsheet view" for your database. Instead of writing SQL queries to see who has registered, you can open Prisma Studio and see a nice table of all your users.
#### What you can do with it:
*   **View Data**: See all rows in your `User` table.
*   **Edit Data**: Double-click a cell (like an email or alias) to change it directly.
*   **Create/Delete**: Add new test users or delete existing ones with a click.
*   **Filter/Sort**: Search for specific users.

To use prisma studio: `make studio` or `cd backend && npx prisma studio`

It will usually open a new tab in your browser (or give you a link like `http://localhost:5555`) where you can interact with your SQLite database visually. It's extremely useful for debugging and checking if your registration logic is working.

---
## Migration
A **Migration** is like version control (Git) for your database structure.

Imagine you have a `User` table, and your app is live. Later, you decide you want to add a `phoneNumber` field.

*   **Without Migrations**: You might manually run a SQL command `ALTER TABLE User ADD phoneNumber...`. But if your colleague tries to run the code, their database will crash because it doesn't have that column yet.
*   **With Migrations**:
    1.  You add `phoneNumber String?` to your schema.prisma file.
    2.  You run a command (like `prisma migrate dev`).
    3.  Prisma generates a new folder (e.g., `migrations/20251217_add_phone/`) containing the SQL to make that change.
    4.  When your colleague pulls your code, they run `prisma migrate dev`, and their database is automatically updated to match yours.
### In this project
You can see the history of changes in migrations:
*   `20251130010234_init/`: The initial creation of the database.
*   `20251130122852_remove_salt_field/`: A change where a "salt" field was removed.
*   `20251207163602_add_google_oauth/`: A change to support Google login.

After a `git pull`: Check if there are new migrations. If yes, run `make migrate`.
It ensures everyone's database looks exactly the same.

---
## Swagger
**Swagger** (now often called **OpenAPI**) is a tool that automatically generates interactive documentation for your API.

Instead of reading code to figure out "How do I register a user?", you can open a webpage that lists every available API endpoint, what parameters it needs, and what it returns.

### In this project
The backend uses `@fastify/swagger` and `@fastify/swagger-ui`. Swagger UI (the library @fastify/swagger-ui) automatically built that page for you. It reads your code and organizes the endpoints based on how they are "tagged" in your route definitions.

1.  **Interactive UI**: If your backend is running, you can visit:
    **`http://localhost:3000/docs`**

2.  **What you will see**:
    *   A list of routes like `POST /auth/register`, `POST /auth/login`.
    *   You can click "Try it out", fill in the form (e.g., email and password), and hit "Execute".
    *   It will send the real request to your backend and show you the response.

Swagger is incredibly useful for testing your backend without needing to build a frontend or write `curl` commands manually.

## My API ?
Yes, **your API** is the code inside the backend folder.

In this project, you are building two main things:
1.  **The Frontend**: The website and the Pong game (what the user sees).
2.  **The Backend (The API)**: The server that handles data, security, and logic (what the user doesn't see).

### What is "Your API"?
It is a set of specific URLs that your Backend listens to. When the Frontend needs to do something (like "Log me in"), it sends a message to one of these URLs.

Based on the file user.route.ts I just read, here are some of the API endpoints **you** (or your colleague) have built:

*   **`POST /users/`**: "Hey API, please create a new user with this email and password."
*   **`POST /users/login`**: "Hey API, here is my password. Is it correct?"
*   **`GET /users/`**: "Hey API, give me a list of all players."

### How it connects
1.  **You** (on the Frontend) click "Register".
2.  **The Frontend** sends a message to `http://localhost:3000/users/`.
3.  **The API** (Backend) receives it, checks the data, and saves it to the Database.
4.  **The API** replies: "Success! User created."

Swagger simply lists all these "commands" (`POST /users/`, `POST /login`) so you can see what your server is capable of doing.  

The backend exposes a REST‚Äëstyle HTTP JSON API (Fastify routes such as POST /users/, GET /users/, etc.)
### REST
REST stands for "Representational State Transfer." It's an architectural style for web APIs that uses standard HTTP methods (GET, POST, PUT, DELETE) to operate on resources (identified by URLs). Key ideas:

- Stateless requests (server doesn't keep client session state)  
- Resources have representations (usually JSON)  
- Use of HTTP methods and status codes to express actions/results  
Example: GET /api/users/123 returns the representation (JSON) of user 123.

An API doesn't have to be REST ‚Äî common alternatives include **GraphQL**, **gRPC**, **WebSockets**, **SSE**, **JSON‚ÄëRPC/RPC**, **message‚Äëbased queues**, and **SOAP**.

### Common alternatives
- **GraphQL** ‚Äî single endpoint, client specifies exactly what fields it needs. Great for complex UIs and reducing round trips.  
- **gRPC** ‚Äî binary, protobuf‚Äëdefined RPC calls (and streaming), high performance and strongly typed; ideal for inter‚Äëservice comms, less browser‚Äëfriendly.  
- **WebSockets** ‚Äî full‚Äëduplex persistent connection for real‚Äëtime two‚Äëway communication (games, chat).  
- **Server‚ÄëSent Events (SSE)** ‚Äî simple server‚Üíclient streaming (live feeds, notifications), one‚Äëway.  
- **JSON‚ÄëRPC / XML‚ÄëRPC** ‚Äî RPC style over HTTP where you call methods rather than CRUD resources. Simple and explicit.  
- **Message queues (AMQP/MQTT/Kafka)** ‚Äî async, decoupled communication for background jobs and event-driven systems.  
- **SOAP / WS‚Äë*** ‚Äî XML, heavy enterprise features (transactions, WS‚ÄëSecurity); rarely used in modern web apps.

### When to choose which üéØ
- Use **REST** for simple, web‚Äënative CRUD and broad compatibility.  
- Use **GraphQL** when clients need flexible queries and to avoid over/under‚Äëfetching.  
- Use **gRPC** for low-latency, typed microservice communication.  
- Use **WebSockets/SSE** for real‚Äëtime updates.  
- Use **message queues** for asynchronous, durable workflows.


---
## Fastify and Vite
No, **Fastify** and **Vite** are **NOT** the same thing. They are two completely different servers doing two different jobs.

This is a very common point of confusion in modern web development. Let me clarify:

**Frontend = Code that runs in the Browser (Client-side).**  
**Backend = Code that runs on the Server (Server-side).**

### 1. The Backend Server (Fastify)
*   **What it is**: A Node.js program running on port `3000`.
*   **File**: app.ts
*   **Job**: It is the **API**.
    *   It talks to the Database (SQLite).
    *   It handles "Logic" (e.g., "Is this password correct?", "Save this score").
    *   It returns **JSON data** (e.g., `{"id": 1, "email": "alice@..."}`).
*   **Analogy**: The **Chef** in the kitchen. He cooks the food (data) but doesn't talk to the customers directly.

### 2. The Frontend Server (Vite)
**When we call Vite the frontend server, it doesn't mean it runs on the frontend. It means that the server is responsible for delivering the frontend files**
*   **What it is**: A development tool running on port `5173`.
*   **File**: vite.config.ts (configuration)
*   **Job**: It serves the **UI(User Interface) files**.
    *   It sends index.html, main.ts, and `style.css` to your browser.
    *   It handles "Hot Reloading" (when you save a file, the browser updates instantly).
    *   It returns **HTML/JS/CSS files**.
*   **Analogy**: The **Waiter**. He brings the menu (HTML) and the plates (JS) to your table (Browser).

### How they work together
1.  **You** (Browser) go to `http://localhost:5173`.
2.  **Vite** sends you the index.html and the JavaScript code (main.ts).
3.  **Your Browser** runs that JavaScript.
4.  When you click "Login", the **JavaScript** (running in your browser) sends a message to `http://localhost:3000/login` (**Fastify**).
5.  **Fastify** checks the password and replies "OK".
6.  The **JavaScript** updates the screen to say "Welcome!".

So, you have **two servers** running at the same time: one for the **Code/UI** (Vite) and one for the **Data/Logic** (Fastify).

Fastify **could** absolutely handle file serving. In fact, in the "old days" (like 10 years ago), this was the standard way to do it: one server did everything.

However, modern web development separates them for several very good reasons:

### 1. Developer Experience (The Main Reason)
*   **Vite** is specialized for *development*.
*   When you change a line of CSS, Vite updates the browser instantly (Hot Module Replacement) without reloading the page.
*   Fastify is not built for this. If Fastify served the files, you would likely have to restart the server or refresh the page manually every time you made a change.

### 2. Performance & Specialization
*   **Vite (Frontend Server)**: Is optimized for bundling, minifying, and serving static assets (images, fonts, JS). In production, this role is often taken over by a dedicated web server like **Nginx** or **Caddy** (which you have in your project!).
*   **Fastify (Backend Server)**: Is optimized for processing logic, talking to databases, and handling security. It shouldn't waste CPU cycles sending logo images to users.

### 3. Decoupling
*   By separating them, you can easily swap things out.
*   Want to build a Mobile App later? It can talk to the same Fastify Backend.
*   Want to rewrite the Frontend in React? You don't need to touch the Backend code.

### In Production (The Twist)
When you deploy this app to the real world (Production):
1.  You run a "build" command. Vite compiles all your TS/CSS into a folder of static HTML/JS files.
2.  You stop running the Vite server.
3.  You use **Caddy** (or Nginx) to serve those static files to users.
4.  Caddy also acts as a "Reverse Proxy," forwarding API requests (like `/api/login`) to your Fastify backend.

So, the separation remains, but the "Frontend Server" changes from Vite (Dev tool) to Caddy (Production web server).

That means there are **two separate programs** running at the same time on your development machine (the server).

1.  **Program 1: The Backend (Fastify)**
    *   **Command**: `nodemon ... node dist/index.js`
    *   **Port**: `3000`
    *   **Language**: Node.js (JavaScript)
    *   **Job**: Waiting for API requests (data).

2.  **Program 2: The Frontend Dev Server (Vite)**
    *   **Command**: `vite`
    *   **Port**: `5173`
    *   **Language**: Node.js (JavaScript)
    *   **Job**: Waiting for file requests (HTML/JS/CSS).

### How do you start both?
When you ran `make dev`, it executed a command (likely `pnpm --parallel run dev`) that started **both** of these programs simultaneously in the same terminal window.

That's why you saw logs from both:
*   `backend dev: Server listening at http://0.0.0.0:3000`
*   `frontend dev: Local: http://localhost:5173/`

They run side-by-side, independently. If the Backend crashes, the Frontend server (Vite) keeps running (you can still see the page, but login won't work). If Vite crashes, the Backend keeps running (API still works, but you can't load the page).

---
## Vite setup
See frontend/vite.config.ts

This is the configuration for the **Vite server**. It's quite simple, but it does a few critical things:

### 1. Network Access (`host: '0.0.0.0'`)
```typescript
host: '0.0.0.0',
```
*   **What it does**: It tells Vite to listen on **all network interfaces**, not just `localhost`.
*   **Why**: This is essential because you are running inside a **Docker container**. If it only listened on `localhost` inside the container, your host machine (your browser) wouldn't be able to reach it.

### 2. Port (`port: 5173`)
```typescript
port: 5173,
```
*   **What it does**: Sets the port number.
*   **Why**: 5173 is the default Vite port, but it's good to be explicit.

### 3. The Proxy (`proxy: { '/api': ... }`)
```typescript
proxy: {
  '/api': {
    target: 'http://localhost:3000',
    changeOrigin: true,
  },
},
```
*   **This is the most important part.**
*   **The Problem**: Your frontend runs on port `5173`. Your backend runs on port `3000`. Browsers block requests between different ports by default (CORS policy) unless configured otherwise. Also, it's annoying to type `http://localhost:3000/api/login` in your code.
*   **The Solution**: This proxy says: "If the frontend asks for anything starting with `/api` (e.g., `/api/login`), **forward** that request to `http://localhost:3000`."
*   **The Benefit**: In your frontend code (main.ts), you can just fetch `/api/login`. You don't need to know *where* the backend is running. Vite handles the handoff.

This setup confirms that your frontend expects the backend to be running on port 3000.

### Where is it launched?

1.  **The Command**:
    In package.json, you can see the script:
    ```json
    "dev": "vite --host 0.0.0.0"
    ```
    This is the command that actually starts the Vite server.

2.  **The Trigger**:
    You (or the `make dev` command) run `pnpm run dev` in the root folder.
    *   The root package.json (which we saw earlier) has `"dev": "pnpm --parallel run dev"`.
    *   This tells pnpm to go into every folder (backend, frontend) and run the `dev` script it finds there.

So, when you type `make dev`:
1.  `make` runs `pnpm run dev`.
2.  `pnpm` sees the workspace has a frontend folder.
3.  `pnpm` runs `vite --host 0.0.0.0` inside the frontend folder.
4.  **Vite starts** and begins serving index.html on port 5173.

### Serving index.html
Vite is "opinionated." By default, it looks for an index.html file in the **root** of the project it is serving (in this case, the frontend folder).

If you look at your file list again:
```
frontend/
	index.html  <-- Vite serves this!
	package.json
	vite.config.ts
    ...
```
It finds it right there. When you request `http://localhost:5173/`, Vite reads that file and sends it to you.

---
## Cookie
A **Cookie** is a small piece of text data that a website (the server) gives to your browser to hold onto.

1.  **Login:** You send your Username/Password to the server.
2.  **Set-Cookie:** If correct, the server sends back a response header: `Set-Cookie: session_id=xyz123; HttpOnly`.
3.  **Storage:** Your browser saves this text file secretly.
4.  **Automatic Sending:** For **every single future request** you make to that server (clicking a link, refreshing the page), your browser automatically attaches that cookie to the request headers.
    *   *Browser:* "Hey Server, give me my profile data. Also, here is my ticket: `session_id=xyz123`."
    *   *Server:* "Ah, ticket `xyz123` belongs to User 'Gentian'. Here is your data."

### Why `HttpOnly`?
In your code, you see the comment `// Uses httpOnly cookies`.
This is a security feature. It means **JavaScript cannot read the cookie**.
*   `document.cookie` returns an empty string.
*   This prevents hackers from stealing your session ticket if they manage to run malicious JavaScript on your page (XSS attacks). Only the browser and the server can see it.

---
## Caddy

**Caddy** is a modern, open-source web server written in Go. It is best known for its ease of use and automatic HTTPS (it can automatically obtain and renew SSL/TLS certificates).

In your project (`ft_transcendence`), Caddy is used as a **Reverse Proxy** and **Web Server** for the production environment.

### How it is used in your workspace:

1.  **Reverse Proxy**:
    *   It sits in front of your application container (`app`) and directs traffic to it.
    *   It exposes ports `80` (HTTP) and `443` (HTTPS) to the outside world, while your app runs internally on port `3000`.

2.  **Configuration (Caddyfile)**:
    *   **Domains**: It is configured to serve `localhost` and `mooo.com`.
    *   **API Routing**: Requests starting with `/api/*` are forwarded to your backend.
    *   **WebSockets**: Requests to `/ws/*` are upgraded and forwarded for real-time features.
    *   **Documentation**: Access to `/docs` (Swagger UI) is restricted to local IP addresses for security.
    *   **Frontend**: All other requests are routed to the app (which serves the frontend).

3.  **Production Setup**:
    *   In docker-compose.prod.yml, the caddy service is defined to run alongside your `app`.
    *   It handles **TLS termination**, meaning it decrypts HTTPS traffic before passing it to your application, simplifying your app's logic.

### Caddy vs Nginx
**Caddy** and **Nginx** are both powerful web servers and reverse proxies, but they have different design philosophies. Here is a quick comparison:

### 1. Ease of Use & Configuration
*   **Caddy**: Famous for its simplicity. The Caddyfile is human-readable and very concise. You can often set up a reverse proxy with just 2-3 lines of config.
*   **Nginx**: Uses a more verbose, block-based configuration style. It is very powerful and flexible but has a steeper learning curve.

### 2. HTTPS / TLS
*   **Caddy**: **Automatic HTTPS by default.** It automatically obtains and renews certificates from Let's Encrypt (or ZeroSSL) without any extra configuration. This is its "killer feature."
*   **Nginx**: Does not handle certificates out of the box. You typically need to set up a separate tool like `certbot` and configure cron jobs for renewal.

### 3. Performance
*   **Caddy**: Written in **Go**. It is fast enough for the vast majority of use cases, but Go's garbage collection can introduce slight latency at extreme scales compared to C.
*   **Nginx**: Written in **C**. It is legendary for its raw performance, low memory footprint, and ability to handle massive concurrency. It is the industry standard for high-load environments.

### 4. Architecture
*   **Caddy**: Memory-safe (thanks to Go). It has a modular architecture where plugins can be compiled in.
*   **Nginx**: Event-driven and asynchronous. It is extremely stable and battle-tested over decades.

### Summary Table

| Feature | Caddy | Nginx |
| :--- | :--- | :--- |
| **Language** | Go | C |
| **Config Style** | Simple, concise (Caddyfile) | Verbose, explicit (`nginx.conf`) |
| **SSL/TLS** | **Automatic** (Built-in) | Manual (requires Certbot/scripts) |
| **Performance** | Excellent (Good enough for most) | **Best-in-class** (High scale) |
| **Binaries** | Single static binary | System package or compiled |

### Which one to choose?
*   **Choose Caddy** if you want **simplicity**, **automatic HTTPS**, and a modern developer experience. It is perfect for side projects, internal tools, and most production apps (like your `ft_transcendence` project).
*   **Choose Nginx** if you need **maximum performance**, have complex caching/load-balancing needs, or are running a high-traffic enterprise infrastructure.

### TLS termination
**TLS Termination** (Transport Layer Security Termination) is the process of decrypting encrypted traffic (HTTPS) at the entry point of your network‚Äîusually a load balancer or reverse proxy‚Äîbefore passing it on to your backend servers as plain text (HTTP).

In your project, **Caddy** performs TLS termination.

### How it works in `ft_transcendence`:

1.  **Encrypted Leg**: A user's browser sends an encrypted HTTPS request to your server (`mooo.com`).
2.  **Termination Point**: Caddy receives this request on port 443. It uses the SSL certificate to **decrypt** the data.
3.  **Unencrypted Leg**: Caddy forwards the now-unencrypted request (HTTP) to your backend app on port 3000.
4.  **Response**: Your app sends a plain HTTP response back to Caddy. Caddy encrypts it and sends it back to the user.

### Why do this?

1.  **Performance**: Decrypting data is CPU-intensive. By letting Caddy handle it (which is written in Go and optimized for this), your Node.js backend is free to focus on application logic.
2.  **Simplicity**: Your backend code doesn't need to know about SSL certificates or encryption keys. It just listens for standard HTTP requests.
3.  **Certificate Management**: Caddy automatically manages the certificates (obtaining and renewing them). If your backend had to do this, you would need complex logic inside your app code.

---
## WebSockets
**WebSockets** is a communication protocol that provides a **full-duplex** communication channel over a single, long-lived connection. WebSockets let the browser keep a single, continuous connection to the server so the game can send/receive updates in real time (no repeated HTTP requests)

Unlike standard HTTP, where the client must always initiate a request to get data ("Request-Response"), WebSockets allow the **server to push data to the client** instantly without the client asking for it.

### Key Differences

*   **HTTP (Standard Web)**:
    *   Client: "Do you have new messages?" -> Server: "No."
    *   Client: "Do you have new messages?" -> Server: "Yes, here is one."
    *   *Good for loading pages, images, and APIs.*

*   **WebSockets (Real-time)**:
    *   Client connects once.
    *   Server: "Here is a new message!" (immediately when it happens)
    *   *Good for games, chat apps, and live dashboards.*

### Why it matters for `ft_transcendence`

Since your project involves a **Pong game**, WebSockets are critical.
1.  **Game State**: When Player A moves their paddle, the server needs to tell Player B *immediately*. HTTP is too slow for this.
2.  **Live Chat**: If you have a chat feature, messages appear instantly.

### In your Caddyfile

I noticed this block in your Caddyfile earlier:

```caddy
	# WebSocket support (for future real-time features)
	handle /ws/* {
		reverse_proxy app:3000 {
			# Required headers for WebSocket upgrade
			header_up Upgrade {http.request.header.Upgrade}
			header_up Connection "Upgrade"
		}
	}
```

This configuration tells Caddy to allow these special long-lived connections to pass through to your backend (`app:3000`) so the game can function in real-time.  
  
Currently, your backend (package.json) does **not** have any WebSocket libraries installed (like `socket.io` or `@fastify/websocket`), and your frontend game code (pong.ts) is designed for a local game loop.

### Why you need them:

1.  **Remote Multiplayer Pong**:
    *   If you want Player A on *Computer 1* to play against Player B on *Computer 2*, you **must** use WebSockets.
    *   HTTP requests are too slow to synchronize the ball and paddle positions 60 times a second.
    *   *Note: If you only plan to support "Local Multiplayer" (two people sharing one keyboard), you might skip this for the game, but check your subject carefully.*

2.  **Real-Time Chat**:
    *   The project usually requires a chat system (Direct Messages, Channels).
    *   Without WebSockets, users would have to refresh the page to see new messages.

3.  **Live Notifications**:
    *   "Player X invited you to play!"
    *   "Player Y is now Online."
    *   These features require the server to push data to the client instantly.

### Recommendation

Since you are using **Fastify**, I recommend installing `@fastify/websocket`. It is lightweight and integrates perfectly with your existing setup.

If you want to start implementing it, I can help you set up the basic structure.

---
## JWT
**JWT** stands for **JSON Web Token**. It is an open standard (RFC 7519) used to securely transmit information between parties as a JSON object.

In web development, it is most commonly used for **stateless authentication**.  
Stateless means the server does not keep any session information about clients between requests. Instead, each request carries everything the server needs to authenticate or authorize it.  

**Stateless = no server-side session data.**  
**Stateful = server stores session (e.g., in memory or DB) and the client only sends an ID.**

### How it works
A JWT is a string composed of three parts, separated by dots (`.`):
1.  **Header**: Specifies the algorithm used (e.g., HS256).
2.  **Payload**: Contains the data (claims) you want to store, like the user's ID or email.
3.  **Signature**: A cryptographic signature used to verify that the token hasn't been tampered with.

When a user logs in, the server creates a JWT signed with a **secret key** and sends it to the client. The client sends this token back with **every** request (usually in a Cookie or Authorization header) to prove who they are.

### How it is used in your project
Based on your app.ts and `.env` files:

1.  **Configuration**:
    The server uses the `fastify-jwt` plugin. It signs tokens using the `JWT_SECRET` defined in your `.env` file:
    ```typescript
    // backend/src/app.ts
    await server.register(fastifyJwt, {
      secret: JWT_SECRET,
      cookie: {
        cookieName: 'token', // The token is stored in a cookie named 'token'
        signed: false,
      },
    });
    ```

2.  **Authentication**:
    There is a helper called `authenticate` that checks if a valid token exists on the request. If the signature is valid, the server trusts the user information inside it.
    ```typescript
    // backend/src/app.ts
    server.decorate('authenticate', async function (request, reply) {
      try {
        await request.jwtVerify(); // Verifies the token
      } catch (err) {
        // ... handles errors
      }
    });
    ```

3.  **Security**:
    The `JWT_SECRET` in your `.env` (`"your-super-secret-jwt-key-change-in-production"`) is the key used to sign these tokens. If someone else gets this key, they can forge fake tokens and impersonate any user. That is why the code warns you to change it in production.

---
## CI/CD
CI/CD stands for **Continuous Integration** and **Continuous Delivery** (or Continuous Deployment). It is a method to frequently deliver apps to customers by introducing automation into the stages of app development.

In the context of this project (as described in DEVCONTAINER-WORKFLOW.md and CI-QUICK-REF.md), here is what it means practically:

### 1. CI (Continuous Integration)
This is the "Quality Gate" mentioned in the docs. Every time you push code to GitHub (specifically to a Pull Request or the `main` branch), an automated system (GitHub Actions) wakes up and runs a series of checks to ensure your new code doesn't break anything.

*   **What happens:**
    *   **Linting:** Checks if your code follows the style rules (e.g., indentation, variable naming).
    *   **Testing:** Runs all your tests (`make test`). If coverage drops below 60%, it fails.
    *   **Building:** Tries to compile/build the project (`make build`) to ensure there are no syntax errors.

*   **Goal:** To catch bugs *automatically* and *early* before they are merged into the main codebase.

### 2. CD (Continuous Delivery/Deployment)
This is the "Deployment" part. Once the CI checks pass and the code is merged into the `main` branch, the system automatically packages your application so it's ready to run on a server.

*   **What happens:**
    *   **Docker Image Build:** It builds a production-ready Docker image of your app.
    *   **Registry Push:** It pushes this image to the GitHub Container Registry (`ghcr.io`).
    *   **Deployment (Optional/Setup dependent):** In many setups, this image is then automatically pulled and run on a live server.

*   **Goal:** To make releasing new versions of the software fast, automated, and reliable.

### Summary for You
*   **You do the work:** Write code, run `make all` locally.
*   **CI checks your work:** GitHub Actions double-checks your work when you push.
*   **CD ships your work:** If CI is happy, the system packages your code for production.

---
## Pre-commit Hooks
**Pre-commit hooks** are scripts that run automatically every time you try to commit changes in Git (when you run `git commit`).

### How they normally work
In a traditional setup, if you try to commit code that has errors (like bad formatting or a syntax error), the "hook" script intercepts the commit and stops it.
*   **Scenario:** You type `git commit -m "fix bug"`.
*   **Hook runs:** It runs a linter (like ESLint) or tests.
*   **Result:**
    *   If checks pass: The commit succeeds.
    *   If checks fail: The commit is blocked, and you see an error message.

### In this project
According to your DEVCONTAINER-WORKFLOW.md, this project **does not use pre-commit hooks** on the host machine.

**Why?**
1.  **Dependency Hell:** To run a hook (e.g., "run eslint"), your host computer would need Node.js and ESLint installed. This project follows the "DevContainer Philosophy" where the host should be clean and only have Docker/Git.
2.  **Performance:** Hooks can make committing slow.
3.  **Alternative:** Instead of an automatic hook, you are expected to manually run `make all` (or `make lint`) inside the container before you commit. The ultimate check is done by the CI/CD pipeline on GitHub.

---
## Coverage requirements
refer to the percentage of your code that is executed by your automated tests. It's a metric used to measure how well your code is tested.

In your project (as defined in CI-QUICK-REF.md), the requirement is:

> **‚â• 60% Line Coverage**

### What this means:
If you write 100 lines of code (logic), your tests must run/execute at least 60 of those lines.

*   **Example:**
    *   You write a function with an `if` statement and an `else` statement.
    *   Your test only checks the `if` case.
    *   You have tested ~50% of that function.
    *   **Result:** You need to write another test for the `else` case to reach 100% (or at least pass the 60% threshold).

### Why it matters:
*   **Quality Gate:** If your Pull Request drops the coverage below 60% (for either backend or frontend), the **CI pipeline will fail**, and you won't be able to merge your code.
*   **Safety:** It ensures that new features are actually being tested, reducing the chance of bugs.

### How to check it:
Run this command inside your DevContainer:
```bash
make test
```
It will print a report showing the coverage percentage for each file and the total.

---
## Dockerfile stages
Builder and Development stages are very similar. They both contain the tools needed to build the software.

The key difference is **who** they are for:

### 1. Builder Stage (`builder`) = For Robots (CI/CD)
*   **Goal:** Make *verifying* the code fast and reproducible.
*   **Includes:**
    *   **Only the essentials:** Node.js, pnpm, compilers.
    *   **NO** VS Code, **NO** fancy terminal plugins, **NO** git credentials.
*   **Context:** This is a "headless" environment. A robot spins it up, runs a command (like `make test`), and then destroys it immediately.

### 2. Development Stage (`development`) = For Humans (You)
*   **Goal:** Make *coding* easy and comfortable.
*   **Includes:**
    *   **VS Code Server:** So you can connect your editor.
    *   **Git:** So you can commit and push.
    *   **Zsh/Bash with plugins:** To make the terminal look nice.
    *   **Debugging tools:** To step through code line-by-line.
    *   **Hot Reloading:** So the app updates instantly when you save.
*   **Context:** This is your "interactive" environment. You live here.

**Why separate them?**
*   **Security:** The Builder stage doesn't need your SSH keys or git config.
*   **Speed:** The Builder stage might be slightly smaller because it doesn't have the "human comfort" tools.
*   **Cleanliness:** The Builder stage guarantees that the tests pass in a pure environment, not just because "it works on my machine" (where you might have tweaked some settings).

### 3. Production Stage (`production`)
*   **What's inside:** Only the finished product. No tools, no blueprints, no scraps. Just the compiled code needed to run the app.
*   **Purpose:** To be shipped to the customer (deployed to the server). It is small, fast, and secure (~300MB).
*   **When used:** When the app is actually running live for users.

The production artifact is a **Docker image** containing the runtime + built files. (Here, runtime means he software environment that actually runs your program, Node.js)

#### Concretely ‚Äî what‚Äôs in a production Docker image for this project
- **Base runtime image** (e.g., `node:18-alpine` or a minimal distro).  
- **Built application files** ‚Äî compiled/transpiled output (`dist/` or `build/`).  
- **Production dependencies** (`node_modules` installed with only runtime deps).  
- **Generated artifacts** (Prisma client, static assets, generated swagger files, etc.).  
- **Entrypoint / CMD** that starts the process (e.g., `node dist/index.js`).  
- **Minimal config / env hooks** (often reads env vars at runtime; secrets should NOT be baked into the image).

#### When *is* it a single binary?
- If the app is written in a compiled language (Go, Rust, C/C++), the builder stage produces a single executable binary which gets copied into the final image. That final image can be tiny (even `scratch`).

**Summary:**
You work in **Development**. The CI uses **Builder** to check your work. The users use **Production**.

---

## Github Actions
**GitHub Actions** is a feature of GitHub that lets you automate your software workflows. It's the tool used for CI (Continuous Integration) in this project.

Think of it as a **robot butler** that lives in your GitHub repository. You give it a list of instructions (the ci.yml file), and it performs them automatically whenever a specific event happens (like when you push code).

### What it does in your project:
1.  **Listens:** It waits for you to push code to the `main` branch or open a Pull Request.
2.  **Wakes up:** When that happens, it spins up a virtual computer (a "runner").
3.  **Works:** It follows your instructions:
    *   Downloads your code.
    *   Builds the Docker container.
    *   Runs your tests (`make test`).
    *   Checks your code style (`make lint`).
4.  **Reports:**
    *   If everything is good, it gives you a green checkmark ‚úÖ.
    *   If something fails, it gives you a red cross ‚ùå and sends you an email.

### Why use it?
*   **Automation:** You don't have to manually run tests on your computer every time you want to share code.
*   **Consistency:** It runs in a clean environment every time, so you avoid "it works on my machine" problems.
*   **Safety:** It prevents bad code from being merged into your main project.  

The robot is just there to double-check and ensure no one forgets to run the tests before merging.  
In an ideal world, everyone would run `make all` before pushing, and we wouldn't need CI. But in reality:
1.  **People forget:** Even smart developers forget sometimes when they are in a rush.
2.  **"It was a small change":** Someone fixes a typo and thinks "I don't need to run tests for this," but accidentally breaks a config file.
3.  **Environment differences:** Sometimes (rarely with Docker, but still) something works for you but fails for others.

The CI robot is the **"enforcer"** that protects the project from mistakes, laziness, and bad luck. It ensures the `main` branch is always clean and working, no matter who pushed the code.

---
## Dependabot
is another automated robot provided by GitHub. Its job is to keep your dependencies (libraries) up to date.

### How it works:
1.  **Scans:** It regularly checks your project (e.g., package.json or Dockerfile).
2.  **Compares:** It looks online to see if there are newer versions of the libraries you use.
3.  **Acts:** If it finds a new version (e.g., React updated from v18.2 to v18.3), it automatically **creates a Pull Request** for you.

### In your project:
According to the dependabot.yml file I just read, it is currently configured to check for updates to your **DevContainer** configuration every week.

*   **Benefit:** You don't have to manually check "Is there a new version of Node.js?" or "Is there a security fix for this library?". Dependabot does it for you.

---
## Run the project on a 42 school computer
The problem is that VS Code is trying to synchronize your host computer's User ID (UID) and Group ID (GID) with the user inside the Docker container (the `node` user). This is done so that files created in the container have the same permissions as your files on the host.

However, on **42 school computers**, the filesystem (often networked or restricted) and security policies frequently prevent Docker from performing `chown` (change ownership) operations. When the container tries to change the owner of its internal files to match your school account's ID, the system rejects it with the **"Invalid argument"** error, which crashes the build.

### How to fix it:
You can tell VS Code to stop trying to sync these IDs by adding this line to your devcontainer.json file:

```json
"updateRemoteUserUID": false
```

This will skip the failing `chown` step and should allow the container to start.


It won't "lock" the folder in the sense of making it inaccessible, but it will likely cause **permission conflicts** between your host machine and the container.

### Why this happens:
1.  **UID Mismatch:** Every user on Linux has a numeric ID (UID). On your host (the school computer), your UID is `102075`. Inside the container, the default `node` user usually has UID `1000`.
2.  **Ownership:** When you create a file inside the container, Linux marks it as owned by UID `1000`. 
3.  **Permission Denied:** When you go back to your host machine and try to edit or delete that same file, the system sees that UID `102075` (you) is trying to modify a file owned by UID `1000` (the container user). Since they don't match, it denies you access.

### In short:
By setting `"updateRemoteUserUID": false`, you stop the container from trying to "become" you. As a result:
*   **Files created by the container** will be owned by the container's user, making them hard to edit from the host.
*   **Files created by you on the host** might be read-only for the container.

On 42 school computers, this is often a necessary trade-off because the network filesystem (NFS) used for student sessions usually forbids Docker from changing file ownership (`chown`), which is what caused your initial crash.

To make the folder on the host modifiable, run this command inside the container:
`sudo chmod -R 777 .`

---
## Pro handling of a Pull Request
Here is how professional teams typically handle this:

### 1. Before the Merge (The Reviewers)
The **Reviewers** (the people assigned to check the code) must read the description thoroughly. It is their job to understand the "Why" and "How" before they approve the code. A good PR description is a roadmap for the reviewer.

### 2. After the Merge (The Rest of the Team)
For the rest of the team, reading every single PR description in detail is often impossible in large projects. Instead, they follow these practices:

*   **The "Impact" Filter:** Developers usually skim PR titles or descriptions for changes that affect **their specific area**. If you are working on the Frontend and someone merges a major change to the Backend API, you *must* read that PR to see if your code will break.
*   **Automated Notifications:** Teams often have a Slack or Discord channel where GitHub posts every merge. Developers skim these notifications to stay "directionally aware" of where the project is going.
*   **The Daily Stand-up:** Instead of reading every PR, teams use a 15-minute daily meeting to say: *"I merged the new Auth system yesterday; check the PR description if you need to see how the new login flow works."*

### 3. The PR as a "Historical Record"
Professionals treat PR descriptions as **permanent documentation**. 
*   If a bug appears 6 months from now, a developer will use `git blame` to find the commit, then find the PR that introduced it. 
*   At that moment, the PR description becomes the most important document in the world because it explains **why** a certain decision was made.

### Summary of Professional Etiquette:
*   **Author:** Write a clear, detailed description (it's your "defense" of the code).
*   **Reviewer:** Read everything before approving.
*   **Teammate:** Skim the titles/summaries to ensure you aren't blindsided by changes to shared files or APIs.

## pnpm
pnpm (Performant npm) is an alternative package manager for Node.js designed to be faster and more disk-efficient than npm or Yarn.

### Core Architecture

Unlike npm, which duplicates packages in every project's node_modules, pnpm uses a content-addressable store to keep a single copy of each package version on your machine.

- Hard Links: pnpm stores all package files in a global store (~/.pnpm-store) and creates hard links to them in your project's node_modules/.pnpm folder.

 -Symlinks: It then creates symbolic links (symlinks) to build the node_modules structure. This ensures your code can only access the dependencies explicitly listed in your package.json.

### Key Advantages

- Disk Efficiency: If you have 100 projects using the same version of React, pnpm saves it once. This can save gigabytes of space.

- Speed: Installations are significantly faster because pnpm avoids re-downloading or re-extracting files already present in the global store.

- Strictness: It prevents "phantom dependencies"‚Äîa situation where you can import a package that you didn't explicitly install because it was "hoisted" by npm. If it's not in your package.json, pnpm won't let you use it.

- Monorepo Support: It has built-in, first-class support for workspaces, making it the industry standard for managing multi-package repositories.
